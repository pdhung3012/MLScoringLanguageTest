Result for GaussianNB(priors=None, var_smoothing=1e-09)
0.47056155294253027
[[1802  449  105 1069]
 [1670  542  270 1713]
 [  98   85 3028  985]
 [1240  597  955 2850]]
              precision    recall  f1-score   support

        A-MF       0.37      0.53      0.44      3425
        A-MM       0.32      0.13      0.18      4195
        A-NE       0.69      0.72      0.71      4196
        A-SE       0.43      0.51      0.46      5642

   micro avg       0.47      0.47      0.47     17458
   macro avg       0.46      0.47      0.45     17458
weighted avg       0.46      0.47      0.45     17458

Result for LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=1234, solver='warn',
          tol=0.0001, verbose=0, warm_start=False)
0.5017168692689379
[[ 772  463  125 2065]
 [ 382  652  325 2836]
 [  10   13 3260  913]
 [ 164  293 1118 4067]]
              precision    recall  f1-score   support

        A-MF       0.58      0.23      0.32      3425
        A-MM       0.46      0.16      0.23      4195
        A-NE       0.68      0.78      0.72      4196
        A-SE       0.41      0.72      0.52      5642

   micro avg       0.50      0.50      0.50     17458
   macro avg       0.53      0.47      0.45     17458
weighted avg       0.52      0.50      0.46     17458

Result for DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
0.5874667374305146
[[1772  793   98  762]
 [ 784 2064  278 1069]
 [ 103  315 3240  538]
 [ 811 1099  593 3139]]
              precision    recall  f1-score   support

        A-MF       0.51      0.52      0.51      3425
        A-MM       0.48      0.49      0.49      4195
        A-NE       0.77      0.77      0.77      4196
        A-SE       0.57      0.56      0.56      5642

   micro avg       0.59      0.59      0.59     17458
   macro avg       0.58      0.58      0.58     17458
weighted avg       0.59      0.59      0.59     17458

Result for RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None,
            oob_score=False, random_state=1234, verbose=0,
            warm_start=False)
0.6721256454249225
[[1994  663   54  714]
 [ 607 2403  217  968]
 [  67  186 3499  444]
 [ 528  843  413 3858]]
              precision    recall  f1-score   support

        A-MF       0.62      0.58      0.60      3425
        A-MM       0.59      0.57      0.58      4195
        A-NE       0.84      0.83      0.84      4196
        A-SE       0.64      0.68      0.66      5642

   micro avg       0.67      0.67      0.67     17458
   macro avg       0.67      0.67      0.67     17458
weighted avg       0.67      0.67      0.67     17458

Result for AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,
          learning_rate=1.0, n_estimators=50, random_state=None)
0.5415324107587722
[[1429  873   44 1079]
 [ 863 1039  193 2100]
 [   8   28 3308  852]
 [ 403  811  777 3651]]
              precision    recall  f1-score   support

        A-MF       0.53      0.42      0.47      3425
        A-MM       0.38      0.25      0.30      4195
        A-NE       0.77      0.79      0.78      4196
        A-SE       0.48      0.65      0.55      5642

   micro avg       0.54      0.54      0.54     17458
   macro avg       0.54      0.53      0.52     17458
weighted avg       0.53      0.54      0.53     17458

Result for LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,
              solver='svd', store_covariance=False, tol=0.0001)
0.6549953921023574
[[1972  603   16  834]
 [ 628 2074  152 1341]
 [  50  144 3272  730]
 [ 405  830  303 4104]]
              precision    recall  f1-score   support

        A-MF       0.65      0.58      0.61      3425
        A-MM       0.57      0.49      0.53      4195
        A-NE       0.87      0.78      0.82      4196
        A-SE       0.59      0.73      0.65      5642

   micro avg       0.65      0.65      0.65     17458
   macro avg       0.67      0.64      0.65     17458
weighted avg       0.66      0.65      0.65     17458

Result for QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
               store_covariance=False, store_covariances=None, tol=0.0001)
0.6663985902073266
[[1928  557   28  912]
 [ 570 2103  174 1348]
 [  51  209 3293  643]
 [ 323  678  326 4315]]
              precision    recall  f1-score   support

        A-MF       0.67      0.56      0.61      3425
        A-MM       0.59      0.50      0.54      4195
        A-NE       0.86      0.78      0.82      4196
        A-SE       0.60      0.76      0.67      5642

   micro avg       0.67      0.67      0.67     17458
   macro avg       0.68      0.65      0.66     17458
weighted avg       0.67      0.67      0.66     17458

Result for LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=1234, tol=0.0001,
     verbose=0)
0.6054593135426292
[[1767  354  128 1176]
 [ 652 1225  362 1956]
 [  19   28 3716  433]
 [ 289  324 1183 3846]]
              precision    recall  f1-score   support

        A-MF       0.65      0.52      0.57      3425
        A-MM       0.63      0.29      0.40      4195
        A-NE       0.69      0.89      0.78      4196
        A-SE       0.52      0.68      0.59      5642

   micro avg       0.60      0.60      0.60     17458
   macro avg       0.62      0.59      0.58     17458
weighted avg       0.61      0.60      0.59     17458

Result for MLPClassifier(activation='relu', alpha=1, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(100,), learning_rate='constant',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
       random_state=None, shuffle=True, solver='adam', tol=0.0001,
       validation_fraction=0.1, verbose=False, warm_start=False)
0.47605386685801737
[[ 471  818   84 2052]
 [ 392  749  234 2820]
 [  17   29 3033 1117]
 [ 229  520  877 4016]]
              precision    recall  f1-score   support

        A-MF       0.42      0.14      0.21      3425
        A-MM       0.35      0.18      0.24      4195
        A-NE       0.72      0.72      0.72      4196
        A-SE       0.40      0.71      0.51      5642

   micro avg       0.47      0.47      0.47     17458
   macro avg       0.47      0.44      0.42     17458
weighted avg       0.47      0.47      0.44     17458

Result for GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=5,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=1234,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
0.6655338743252927
[[1966  637   50  772]
 [ 584 2202  196 1213]
 [  44  125 3506  521]
 [ 458  813  422 3949]]
              precision    recall  f1-score   support

        A-MF       0.64      0.57      0.61      3425
        A-MM       0.58      0.52      0.55      4195
        A-NE       0.84      0.84      0.84      4196
        A-SE       0.61      0.70      0.65      5642

   micro avg       0.67      0.67      0.67     17458
   macro avg       0.67      0.66      0.66     17458
weighted avg       0.67      0.67      0.66     17458

