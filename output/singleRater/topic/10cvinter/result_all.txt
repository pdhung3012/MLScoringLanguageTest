Result for GaussianNB(priors=None, var_smoothing=1e-09)
0.6085383419256485
[[712   0   1   7   1  11 139  83   0  18  25   3   1]
 [  0  52   0   0   0   0   7   3   0   1   0   0   0]
 [ 33   1 358  36  40  80 152 125   0 110  50   8   0]
 [  3   1   0 395   0   1  27  87   0  11  16   4   0]
 [  3   2  14   0 556  13  32  56   0  15  50   5   1]
 [ 17   1  17   9   5 429 189 107   0 242  45  34   1]
 [  8   0   1   0   5   2 156   2   0   1  11   0   0]
 [  4   3   0  33  12   9 124 652   0  35  33   1   1]
 [  0   0   2   0   8   1   1   0  43   0   6   0   0]
 [  8   0   0   2  23 140  46  41   0 443  14  10   1]
 [  0   0   0   1  17   1  40   6   0   0 124   0   0]
 [  3   0   0   5   3  26  65  59   0  40  19 367   1]
 [  9   2   0   4   1  16  79  56   0  13   7   5 393]]
                    precision    recall  f1-score   support

           ANIMALS       0.89      0.71      0.79      1001
           CAREERS       0.84      0.83      0.83        63
    COMMUNICATIONS       0.91      0.36      0.52       993
        DIRECTIONS       0.80      0.72      0.76       545
    Favorite Class       0.83      0.74      0.78       747
       POP_CULTURE       0.59      0.39      0.47      1096
           Project       0.15      0.84      0.25       186
        Questions        0.51      0.72      0.60       907
           SCIENCE       1.00      0.70      0.83        61
School Improvement       0.48      0.61      0.53       728
    School Routine       0.31      0.66      0.42       189
           THEATER       0.84      0.62      0.72       588
            TRAVEL       0.98      0.67      0.80       585

         micro avg       0.61      0.61      0.61      7689
         macro avg       0.70      0.66      0.64      7689
      weighted avg       0.73      0.61      0.63      7689

Result for LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=1234, solver='warn',
          tol=0.0001, verbose=0, warm_start=False)
0.7765756669823413
[[ 890    0    5    0    4   90    0   10    0    0    0    0    2]
 [  37    0   10    0    2    2    0    6    0    6    0    0    0]
 [   8    0  757    0   28  166    0   25    0    9    0    0    0]
 [   8    0   24  432    0   50    0   27    0    2    0    1    1]
 [   2    0   13    0  636   75    0   11    0    8    0    1    1]
 [   3    0   25    0    4 1033    0    9    0   21    0    0    1]
 [  37    0   29    2    8   93    0   14    0    3    0    0    0]
 [   4    0   10    0   12   85    0  789    0    6    0    0    1]
 [   2    0   22    0   32    4    0    0    0    1    0    0    0]
 [   6    0    6    0   22  140    0    7    0  546    0    0    1]
 [   1    0   12    2   29  112    0    5    0    3   24    0    1]
 [   6    0    6    1    2  139    0   12    0    4    0  416    2]
 [   5    0   17    0    1   78    0   21    0   13    0    4  446]]
                    precision    recall  f1-score   support

           ANIMALS       0.88      0.89      0.89      1001
           CAREERS       0.00      0.00      0.00        63
    COMMUNICATIONS       0.81      0.76      0.78       993
        DIRECTIONS       0.99      0.79      0.88       545
    Favorite Class       0.82      0.85      0.83       747
       POP_CULTURE       0.50      0.94      0.65      1096
           Project       0.00      0.00      0.00       186
        Questions        0.84      0.87      0.86       907
           SCIENCE       0.00      0.00      0.00        61
School Improvement       0.88      0.75      0.81       728
    School Routine       1.00      0.13      0.23       189
           THEATER       0.99      0.71      0.82       588
            TRAVEL       0.98      0.76      0.86       585

         micro avg       0.78      0.78      0.78      7689
         macro avg       0.67      0.57      0.59      7689
      weighted avg       0.80      0.78      0.76      7689

Result for DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
0.7911477523521364
[[877   2  24  10   4  29   5  15   1  13   6   8   7]
 [  4  30   4   4   6   2   0   5   0   5   1   2   0]
 [ 23   2 725  24  30  97   5  25   8  25   7  11  11]
 [ 10   2  27 450   1  21   0   9   0   5   3   9   8]
 [  2   3  24   2 617  23   1  13   6  35   9   6   6]
 [ 26   3  59  11   6 881   4  24   0  41   9  25   7]
 [  9   4   6   2   6  16 128   8   0   2   2   0   3]
 [ 15   2  29  12  20  51   9 734   0  16   2   9   8]
 [  1   1   4   2   5   5   1   2  35   2   1   2   0]
 [  8   3  20   6  28  58   2  17   1 554   3  16  12]
 [  5   0  13   2   4  22   1   3   0   6 127   5   1]
 [  9   1  18   7  10  50   0  11   0  13   7 453   9]
 [ 12   2   9   6   8  32   1  15   1  19   3   4 473]]
                    precision    recall  f1-score   support

           ANIMALS       0.88      0.88      0.88      1001
           CAREERS       0.55      0.48      0.51        63
    COMMUNICATIONS       0.75      0.73      0.74       993
        DIRECTIONS       0.84      0.83      0.83       545
    Favorite Class       0.83      0.83      0.83       747
       POP_CULTURE       0.68      0.80      0.74      1096
           Project       0.82      0.69      0.75       186
        Questions        0.83      0.81      0.82       907
           SCIENCE       0.67      0.57      0.62        61
School Improvement       0.75      0.76      0.76       728
    School Routine       0.71      0.67      0.69       189
           THEATER       0.82      0.77      0.80       588
            TRAVEL       0.87      0.81      0.84       585

         micro avg       0.79      0.79      0.79      7689
         macro avg       0.77      0.74      0.75      7689
      weighted avg       0.79      0.79      0.79      7689

